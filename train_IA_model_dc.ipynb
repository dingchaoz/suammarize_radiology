{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Install\n",
    "```\n",
    "pip install torch torchvision --user\n",
    "pip install tqdm --user\n",
    "pip install git+https://github.com/tagucci/pythonrouge.git\n",
    "```\n",
    "\n",
    "If `pythonrouge` gives an error about `perl` saying `non-zero exit status 2`:\n",
    "```\n",
    "apt-get install libxml-parser-perl\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7M\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu 1.7M May 10 19:01 \u001b[0m\u001b[01;32mConvert_IA_data_to_jsonl.ipynb\u001b[0m*\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  19K May 27 02:12 train_IA_model_dc.ipynb\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  19K Apr 16 15:58 \u001b[01;32mtrain_IA_model.ipynb\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check GPU Presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of GPUs:  1\n",
      "GPU index:  0\n",
      "GPU name:   Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "ind = torch.cuda.current_device()\n",
    "torch.cuda.device(ind)\n",
    "print(\"# of GPUs:  {}\".format(torch.cuda.device_count()))\n",
    "print(\"GPU index:  {}\".format(ind))\n",
    "print(\"GPU name:   {}\".format(torch.cuda.get_device_name(ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Word Embeddings and Vocabulary\n",
    "- Create a folder called `ia-patients` under `dataset`\n",
    "- Converted `jsonl` data goes to this folder\n",
    "- 3 files needed: `train.jsonl`, `dev.jsonl`, `test.jsonl`\n",
    "- More training parameters are available in `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run ../prepare_vocab.py ../dataset/ia-patients/ ../dataset/vocab --glove_dir ../dataset/glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 417194 loaded from file\n",
      "Loading data from dataset/ia-patients/utku_reduced_data/ with batch size 2...\n",
      "74648 batches created for dataset/ia-patients/utku_reduced_data//trainu.jsonl.\n",
      "9331 batches created for dataset/ia-patients/utku_reduced_data//devu.jsonl.\n",
      "Config saved to file ./saved_models/IA_Model_v1/config.json\n",
      "Overwriting old vocab file at ./saved_models/IA_Model_v1/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/ia-patients/utku_reduced_data/\n",
      "\tvocab_dir : dataset/vocab\n",
      "\thidden_dim : 200\n",
      "\temb_dim : 300\n",
      "\tnum_layers : 2\n",
      "\temb_dropout : 0.5\n",
      "\tdropout : 0.5\n",
      "\tlower : True\n",
      "\tmax_dec_len : 80\n",
      "\tbeam_size : 5\n",
      "\ttop : 1000000\n",
      "\ttrain_data : trainu\n",
      "\tdev_data : devu\n",
      "\tattn_type : mlp\n",
      "\tcov : False\n",
      "\tcov_alpha : 0\n",
      "\tcov_loss_epoch : 0\n",
      "\tbackground : True\n",
      "\tconcat_background : False\n",
      "\tuse_bleu : False\n",
      "\tsample_train : 1.0\n",
      "\tlr : 0.001\n",
      "\tlr_decay : 0.9\n",
      "\tdecay_epoch : 15\n",
      "\toptim : adam\n",
      "\tnum_epoch : 3\n",
      "\tbatch_size : 2\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : IA_Model_v1\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tvocab_size : 417194\n",
      "\tmodel_save_dir : ./saved_models/IA_Model_v1\n",
      "\n",
      "\n",
      "Building Seq2Seq with Copy model ...\n",
      "Using mlp attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Update coverage loss weight to be 0]\n",
      "2019-05-28 03:50:50: step 20/223944 (epoch 1/3), loss = 8.739651 (0.225 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:50:55: step 40/223944 (epoch 1/3), loss = 6.498991 (0.187 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:50:59: step 60/223944 (epoch 1/3), loss = 7.074767 (0.183 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:03: step 80/223944 (epoch 1/3), loss = 7.407170 (0.227 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:08: step 100/223944 (epoch 1/3), loss = 7.810974 (0.139 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:12: step 120/223944 (epoch 1/3), loss = 8.307591 (0.293 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:16: step 140/223944 (epoch 1/3), loss = 7.522011 (0.162 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:21: step 160/223944 (epoch 1/3), loss = 7.940779 (0.186 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:25: step 180/223944 (epoch 1/3), loss = 7.846244 (0.110 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:29: step 200/223944 (epoch 1/3), loss = 8.106845 (0.300 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:33: step 220/223944 (epoch 1/3), loss = 7.840403 (0.439 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:37: step 240/223944 (epoch 1/3), loss = 7.079335 (0.093 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:41: step 260/223944 (epoch 1/3), loss = 6.144630 (0.242 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:44: step 280/223944 (epoch 1/3), loss = 6.765625 (0.127 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:49: step 300/223944 (epoch 1/3), loss = 6.349009 (0.098 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:53: step 320/223944 (epoch 1/3), loss = 6.101510 (0.144 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:51:57: step 340/223944 (epoch 1/3), loss = 4.753158 (0.130 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:00: step 360/223944 (epoch 1/3), loss = 7.079018 (0.148 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:04: step 380/223944 (epoch 1/3), loss = 5.760486 (0.129 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:09: step 400/223944 (epoch 1/3), loss = 6.386063 (0.161 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:12: step 420/223944 (epoch 1/3), loss = 6.113611 (0.183 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:16: step 440/223944 (epoch 1/3), loss = 7.086542 (0.254 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:20: step 460/223944 (epoch 1/3), loss = 7.845630 (0.236 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:24: step 480/223944 (epoch 1/3), loss = 7.439916 (0.140 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:28: step 500/223944 (epoch 1/3), loss = 6.931635 (0.176 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:32: step 520/223944 (epoch 1/3), loss = 6.780346 (0.325 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:36: step 540/223944 (epoch 1/3), loss = 7.768677 (0.168 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:40: step 560/223944 (epoch 1/3), loss = 6.823081 (0.133 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:44: step 580/223944 (epoch 1/3), loss = 6.265943 (0.184 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:47: step 600/223944 (epoch 1/3), loss = 5.696064 (0.172 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:51: step 620/223944 (epoch 1/3), loss = 6.456209 (0.230 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:55: step 640/223944 (epoch 1/3), loss = 6.048691 (0.147 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:52:59: step 660/223944 (epoch 1/3), loss = 7.474396 (0.209 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:03: step 680/223944 (epoch 1/3), loss = 6.502911 (0.255 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:07: step 700/223944 (epoch 1/3), loss = 6.093224 (0.194 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:10: step 720/223944 (epoch 1/3), loss = 6.303158 (0.240 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:14: step 740/223944 (epoch 1/3), loss = 7.015073 (0.207 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:18: step 760/223944 (epoch 1/3), loss = 6.470024 (0.139 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:23: step 780/223944 (epoch 1/3), loss = 6.042009 (0.346 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:27: step 800/223944 (epoch 1/3), loss = 5.899779 (0.178 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:31: step 820/223944 (epoch 1/3), loss = 6.299380 (0.147 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:35: step 840/223944 (epoch 1/3), loss = 6.800429 (0.420 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:39: step 860/223944 (epoch 1/3), loss = 6.150554 (0.367 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:43: step 880/223944 (epoch 1/3), loss = 5.877757 (0.191 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:47: step 900/223944 (epoch 1/3), loss = 5.552527 (0.189 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:51: step 920/223944 (epoch 1/3), loss = 6.667788 (0.229 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:55: step 940/223944 (epoch 1/3), loss = 6.833376 (0.192 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:53:59: step 960/223944 (epoch 1/3), loss = 4.960104 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:03: step 980/223944 (epoch 1/3), loss = 7.102294 (0.259 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:07: step 1000/223944 (epoch 1/3), loss = 5.224866 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:11: step 1020/223944 (epoch 1/3), loss = 7.352696 (0.261 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:14: step 1040/223944 (epoch 1/3), loss = 5.807305 (0.175 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:17: step 1060/223944 (epoch 1/3), loss = 5.873940 (0.149 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:22: step 1080/223944 (epoch 1/3), loss = 7.114506 (0.539 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:26: step 1100/223944 (epoch 1/3), loss = 7.434380 (0.146 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:30: step 1120/223944 (epoch 1/3), loss = 6.647979 (0.477 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:34: step 1140/223944 (epoch 1/3), loss = 6.345847 (0.267 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:38: step 1160/223944 (epoch 1/3), loss = 6.435136 (0.139 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:42: step 1180/223944 (epoch 1/3), loss = 5.703133 (0.205 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:46: step 1200/223944 (epoch 1/3), loss = 6.627556 (0.334 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:51: step 1220/223944 (epoch 1/3), loss = 7.359418 (0.411 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:55: step 1240/223944 (epoch 1/3), loss = 5.898453 (0.184 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:54:59: step 1260/223944 (epoch 1/3), loss = 6.761573 (0.257 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:02: step 1280/223944 (epoch 1/3), loss = 5.064605 (0.216 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:06: step 1300/223944 (epoch 1/3), loss = 6.533303 (0.280 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:10: step 1320/223944 (epoch 1/3), loss = 6.437368 (0.150 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:14: step 1340/223944 (epoch 1/3), loss = 6.631824 (0.196 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:18: step 1360/223944 (epoch 1/3), loss = 4.357459 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:22: step 1380/223944 (epoch 1/3), loss = 6.434636 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:27: step 1400/223944 (epoch 1/3), loss = 6.922932 (0.136 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:30: step 1420/223944 (epoch 1/3), loss = 6.509176 (0.184 sec/batch), lr: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 03:55:35: step 1440/223944 (epoch 1/3), loss = 5.137422 (0.187 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:39: step 1460/223944 (epoch 1/3), loss = 5.846762 (0.178 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:43: step 1480/223944 (epoch 1/3), loss = 6.678418 (0.224 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:47: step 1500/223944 (epoch 1/3), loss = 4.164983 (0.102 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:51: step 1520/223944 (epoch 1/3), loss = 6.179486 (0.453 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:55:56: step 1540/223944 (epoch 1/3), loss = 5.960763 (0.224 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:00: step 1560/223944 (epoch 1/3), loss = 6.125066 (0.143 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:05: step 1580/223944 (epoch 1/3), loss = 4.908601 (0.176 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:09: step 1600/223944 (epoch 1/3), loss = 6.873186 (0.516 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:14: step 1620/223944 (epoch 1/3), loss = 4.950609 (0.150 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:18: step 1640/223944 (epoch 1/3), loss = 4.408019 (0.131 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:23: step 1660/223944 (epoch 1/3), loss = 5.611827 (0.225 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:27: step 1680/223944 (epoch 1/3), loss = 5.417361 (0.225 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:30: step 1700/223944 (epoch 1/3), loss = 5.828096 (0.229 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:34: step 1720/223944 (epoch 1/3), loss = 5.479987 (0.183 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:37: step 1740/223944 (epoch 1/3), loss = 5.851024 (0.224 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:41: step 1760/223944 (epoch 1/3), loss = 5.410109 (0.206 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:46: step 1780/223944 (epoch 1/3), loss = 5.448066 (0.248 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:49: step 1800/223944 (epoch 1/3), loss = 5.671567 (0.168 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:54: step 1820/223944 (epoch 1/3), loss = 6.006008 (0.185 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:56:57: step 1840/223944 (epoch 1/3), loss = 6.120074 (0.144 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:01: step 1860/223944 (epoch 1/3), loss = 3.309229 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:04: step 1880/223944 (epoch 1/3), loss = 5.939883 (0.136 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:08: step 1900/223944 (epoch 1/3), loss = 7.096936 (0.132 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:12: step 1920/223944 (epoch 1/3), loss = 6.323229 (0.315 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:17: step 1940/223944 (epoch 1/3), loss = 5.418669 (0.145 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:20: step 1960/223944 (epoch 1/3), loss = 5.169400 (0.106 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:24: step 1980/223944 (epoch 1/3), loss = 6.556072 (0.282 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:28: step 2000/223944 (epoch 1/3), loss = 5.661845 (0.140 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:32: step 2020/223944 (epoch 1/3), loss = 5.574551 (0.095 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:36: step 2040/223944 (epoch 1/3), loss = 6.510798 (0.317 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:40: step 2060/223944 (epoch 1/3), loss = 5.673224 (0.318 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:44: step 2080/223944 (epoch 1/3), loss = 5.348929 (0.220 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:49: step 2100/223944 (epoch 1/3), loss = 6.076946 (0.280 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:54: step 2120/223944 (epoch 1/3), loss = 6.387180 (0.294 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:57:58: step 2140/223944 (epoch 1/3), loss = 5.842859 (0.324 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:02: step 2160/223944 (epoch 1/3), loss = 4.457887 (0.146 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:06: step 2180/223944 (epoch 1/3), loss = 5.112502 (0.179 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:10: step 2200/223944 (epoch 1/3), loss = 5.408967 (0.146 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:13: step 2220/223944 (epoch 1/3), loss = 5.364259 (0.223 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:17: step 2240/223944 (epoch 1/3), loss = 5.557677 (0.160 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:21: step 2260/223944 (epoch 1/3), loss = 4.583667 (0.177 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:25: step 2280/223944 (epoch 1/3), loss = 5.536536 (0.177 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:28: step 2300/223944 (epoch 1/3), loss = 4.753184 (0.132 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:33: step 2320/223944 (epoch 1/3), loss = 6.625369 (0.216 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:37: step 2340/223944 (epoch 1/3), loss = 5.861533 (0.206 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:40: step 2360/223944 (epoch 1/3), loss = 6.142314 (0.281 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:44: step 2380/223944 (epoch 1/3), loss = 6.070763 (0.191 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:48: step 2400/223944 (epoch 1/3), loss = 4.839380 (0.104 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:52: step 2420/223944 (epoch 1/3), loss = 5.637720 (0.111 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:58:56: step 2440/223944 (epoch 1/3), loss = 3.682850 (0.099 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:00: step 2460/223944 (epoch 1/3), loss = 5.543045 (0.254 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:04: step 2480/223944 (epoch 1/3), loss = 6.261354 (0.210 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:08: step 2500/223944 (epoch 1/3), loss = 5.560718 (0.164 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:11: step 2520/223944 (epoch 1/3), loss = 6.394462 (0.225 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:16: step 2540/223944 (epoch 1/3), loss = 5.049314 (0.292 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:19: step 2560/223944 (epoch 1/3), loss = 5.557407 (0.171 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:23: step 2580/223944 (epoch 1/3), loss = 5.426911 (0.133 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:27: step 2600/223944 (epoch 1/3), loss = 5.764664 (0.361 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:32: step 2620/223944 (epoch 1/3), loss = 5.855478 (0.155 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:36: step 2640/223944 (epoch 1/3), loss = 5.429364 (0.270 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:41: step 2660/223944 (epoch 1/3), loss = 5.288274 (0.250 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:45: step 2680/223944 (epoch 1/3), loss = 4.075702 (0.087 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:49: step 2700/223944 (epoch 1/3), loss = 5.869794 (0.217 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:54: step 2720/223944 (epoch 1/3), loss = 5.618752 (0.359 sec/batch), lr: 0.001000\n",
      "2019-05-28 03:59:58: step 2740/223944 (epoch 1/3), loss = 6.145537 (0.207 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:01: step 2760/223944 (epoch 1/3), loss = 4.170591 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:05: step 2780/223944 (epoch 1/3), loss = 6.137609 (0.185 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:09: step 2800/223944 (epoch 1/3), loss = 6.552437 (0.151 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:12: step 2820/223944 (epoch 1/3), loss = 5.842393 (0.289 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:17: step 2840/223944 (epoch 1/3), loss = 5.503823 (0.286 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:21: step 2860/223944 (epoch 1/3), loss = 3.958095 (0.096 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:26: step 2880/223944 (epoch 1/3), loss = 6.242717 (0.357 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:30: step 2900/223944 (epoch 1/3), loss = 5.310366 (0.258 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:33: step 2920/223944 (epoch 1/3), loss = 5.914379 (0.250 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:37: step 2940/223944 (epoch 1/3), loss = 3.880050 (0.115 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:40: step 2960/223944 (epoch 1/3), loss = 4.421422 (0.092 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:45: step 2980/223944 (epoch 1/3), loss = 4.777360 (0.143 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:48: step 3000/223944 (epoch 1/3), loss = 6.425110 (0.162 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:53: step 3020/223944 (epoch 1/3), loss = 5.095599 (0.362 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:00:57: step 3040/223944 (epoch 1/3), loss = 6.145190 (0.109 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:01: step 3060/223944 (epoch 1/3), loss = 4.544734 (0.093 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:04: step 3080/223944 (epoch 1/3), loss = 5.225148 (0.139 sec/batch), lr: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 04:01:09: step 3100/223944 (epoch 1/3), loss = 5.711883 (0.369 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:13: step 3120/223944 (epoch 1/3), loss = 4.896299 (0.153 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:17: step 3140/223944 (epoch 1/3), loss = 7.040502 (0.185 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:20: step 3160/223944 (epoch 1/3), loss = 5.858666 (0.136 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:25: step 3180/223944 (epoch 1/3), loss = 4.296470 (0.152 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:28: step 3200/223944 (epoch 1/3), loss = 5.193908 (0.312 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:32: step 3220/223944 (epoch 1/3), loss = 5.586099 (0.127 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:36: step 3240/223944 (epoch 1/3), loss = 4.987723 (0.198 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:39: step 3260/223944 (epoch 1/3), loss = 5.087118 (0.157 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:44: step 3280/223944 (epoch 1/3), loss = 5.295550 (0.252 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:48: step 3300/223944 (epoch 1/3), loss = 6.555799 (0.240 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:53: step 3320/223944 (epoch 1/3), loss = 5.029257 (0.301 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:01:56: step 3340/223944 (epoch 1/3), loss = 4.024992 (0.167 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:01: step 3360/223944 (epoch 1/3), loss = 4.252769 (0.191 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:06: step 3380/223944 (epoch 1/3), loss = 7.174056 (0.503 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:11: step 3400/223944 (epoch 1/3), loss = 5.467191 (0.377 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:15: step 3420/223944 (epoch 1/3), loss = 5.510266 (0.223 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:20: step 3440/223944 (epoch 1/3), loss = 4.707207 (0.180 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:24: step 3460/223944 (epoch 1/3), loss = 4.379052 (0.116 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:28: step 3480/223944 (epoch 1/3), loss = 4.677215 (0.172 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:31: step 3500/223944 (epoch 1/3), loss = 3.915363 (0.087 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:35: step 3520/223944 (epoch 1/3), loss = 6.924970 (0.241 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:39: step 3540/223944 (epoch 1/3), loss = 3.930419 (0.093 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:44: step 3560/223944 (epoch 1/3), loss = 6.006342 (0.194 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:47: step 3580/223944 (epoch 1/3), loss = 4.295251 (0.148 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:51: step 3600/223944 (epoch 1/3), loss = 4.203626 (0.121 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:55: step 3620/223944 (epoch 1/3), loss = 5.414968 (0.156 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:02:59: step 3640/223944 (epoch 1/3), loss = 5.699596 (0.241 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:04: step 3660/223944 (epoch 1/3), loss = 5.923737 (0.179 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:08: step 3680/223944 (epoch 1/3), loss = 6.031933 (0.317 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:12: step 3700/223944 (epoch 1/3), loss = 6.494407 (0.277 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:15: step 3720/223944 (epoch 1/3), loss = 4.136639 (0.109 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:20: step 3740/223944 (epoch 1/3), loss = 5.665900 (0.126 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:25: step 3760/223944 (epoch 1/3), loss = 4.991782 (0.137 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:28: step 3780/223944 (epoch 1/3), loss = 6.013956 (0.186 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:32: step 3800/223944 (epoch 1/3), loss = 4.208077 (0.083 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:36: step 3820/223944 (epoch 1/3), loss = 6.230406 (0.293 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:40: step 3840/223944 (epoch 1/3), loss = 5.527689 (0.178 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:44: step 3860/223944 (epoch 1/3), loss = 5.108726 (0.113 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:48: step 3880/223944 (epoch 1/3), loss = 5.140829 (0.147 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:53: step 3900/223944 (epoch 1/3), loss = 5.736141 (1.104 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:03:56: step 3920/223944 (epoch 1/3), loss = 5.710990 (0.238 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:00: step 3940/223944 (epoch 1/3), loss = 6.749257 (0.385 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:04: step 3960/223944 (epoch 1/3), loss = 6.041559 (0.285 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:08: step 3980/223944 (epoch 1/3), loss = 5.490140 (0.143 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:12: step 4000/223944 (epoch 1/3), loss = 6.368320 (0.333 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:16: step 4020/223944 (epoch 1/3), loss = 5.416393 (0.113 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:19: step 4040/223944 (epoch 1/3), loss = 5.730629 (0.242 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:23: step 4060/223944 (epoch 1/3), loss = 5.333120 (0.096 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:26: step 4080/223944 (epoch 1/3), loss = 5.622556 (0.186 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:31: step 4100/223944 (epoch 1/3), loss = 5.360479 (0.243 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:35: step 4120/223944 (epoch 1/3), loss = 4.943419 (0.203 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:39: step 4140/223944 (epoch 1/3), loss = 6.124722 (0.330 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:43: step 4160/223944 (epoch 1/3), loss = 5.436704 (0.242 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:46: step 4180/223944 (epoch 1/3), loss = 4.391169 (0.102 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:50: step 4200/223944 (epoch 1/3), loss = 6.779999 (0.286 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:55: step 4220/223944 (epoch 1/3), loss = 5.417714 (0.397 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:04:59: step 4240/223944 (epoch 1/3), loss = 5.950542 (0.209 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:02: step 4260/223944 (epoch 1/3), loss = 4.061480 (0.149 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:06: step 4280/223944 (epoch 1/3), loss = 5.148759 (0.142 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:10: step 4300/223944 (epoch 1/3), loss = 5.147332 (0.178 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:14: step 4320/223944 (epoch 1/3), loss = 4.733107 (0.174 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:19: step 4340/223944 (epoch 1/3), loss = 5.528889 (0.281 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:23: step 4360/223944 (epoch 1/3), loss = 4.444592 (0.086 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:26: step 4380/223944 (epoch 1/3), loss = 5.103000 (0.148 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:31: step 4400/223944 (epoch 1/3), loss = 5.341229 (0.198 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:34: step 4420/223944 (epoch 1/3), loss = 4.382859 (0.110 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:39: step 4440/223944 (epoch 1/3), loss = 5.816650 (0.256 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:43: step 4460/223944 (epoch 1/3), loss = 5.344356 (0.106 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:46: step 4480/223944 (epoch 1/3), loss = 6.193504 (0.186 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:51: step 4500/223944 (epoch 1/3), loss = 3.729838 (0.095 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:54: step 4520/223944 (epoch 1/3), loss = 5.973553 (0.255 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:05:58: step 4540/223944 (epoch 1/3), loss = 4.466939 (0.157 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:01: step 4560/223944 (epoch 1/3), loss = 5.007205 (0.172 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:05: step 4580/223944 (epoch 1/3), loss = 4.663369 (0.141 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:09: step 4600/223944 (epoch 1/3), loss = 6.067698 (0.231 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:14: step 4620/223944 (epoch 1/3), loss = 6.876643 (0.213 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:18: step 4640/223944 (epoch 1/3), loss = 5.803731 (0.184 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:22: step 4660/223944 (epoch 1/3), loss = 5.452497 (0.206 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:26: step 4680/223944 (epoch 1/3), loss = 5.567453 (0.254 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:30: step 4700/223944 (epoch 1/3), loss = 4.659512 (0.091 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:35: step 4720/223944 (epoch 1/3), loss = 4.315162 (0.202 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:39: step 4740/223944 (epoch 1/3), loss = 4.761745 (0.148 sec/batch), lr: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 04:06:42: step 4760/223944 (epoch 1/3), loss = 5.277436 (0.133 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:46: step 4780/223944 (epoch 1/3), loss = 3.849370 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:49: step 4800/223944 (epoch 1/3), loss = 5.439433 (0.252 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:54: step 4820/223944 (epoch 1/3), loss = 5.001100 (0.337 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:06:57: step 4840/223944 (epoch 1/3), loss = 4.090434 (0.158 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:02: step 4860/223944 (epoch 1/3), loss = 5.108887 (0.272 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:05: step 4880/223944 (epoch 1/3), loss = 5.241382 (0.192 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:09: step 4900/223944 (epoch 1/3), loss = 4.954860 (0.171 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:13: step 4920/223944 (epoch 1/3), loss = 5.224489 (0.150 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:17: step 4940/223944 (epoch 1/3), loss = 4.813218 (0.126 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:21: step 4960/223944 (epoch 1/3), loss = 4.567449 (0.150 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:25: step 4980/223944 (epoch 1/3), loss = 4.746764 (0.149 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:28: step 5000/223944 (epoch 1/3), loss = 4.390409 (0.125 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:32: step 5020/223944 (epoch 1/3), loss = 3.392750 (0.171 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:36: step 5040/223944 (epoch 1/3), loss = 5.412337 (0.137 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:40: step 5060/223944 (epoch 1/3), loss = 4.965973 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:44: step 5080/223944 (epoch 1/3), loss = 4.987512 (0.209 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:47: step 5100/223944 (epoch 1/3), loss = 5.302992 (0.180 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:51: step 5120/223944 (epoch 1/3), loss = 5.397523 (0.347 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:07:55: step 5140/223944 (epoch 1/3), loss = 5.535445 (0.193 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:00: step 5160/223944 (epoch 1/3), loss = 6.841303 (0.387 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:04: step 5180/223944 (epoch 1/3), loss = 3.829088 (0.154 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:08: step 5200/223944 (epoch 1/3), loss = 5.939791 (0.201 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:13: step 5220/223944 (epoch 1/3), loss = 5.876737 (0.156 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:17: step 5240/223944 (epoch 1/3), loss = 4.369353 (0.171 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:21: step 5260/223944 (epoch 1/3), loss = 5.066952 (0.200 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:26: step 5280/223944 (epoch 1/3), loss = 5.307479 (0.288 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:29: step 5300/223944 (epoch 1/3), loss = 4.049301 (0.186 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:33: step 5320/223944 (epoch 1/3), loss = 4.807853 (0.124 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:37: step 5340/223944 (epoch 1/3), loss = 5.185337 (0.144 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:41: step 5360/223944 (epoch 1/3), loss = 4.497297 (0.183 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:45: step 5380/223944 (epoch 1/3), loss = 5.192790 (0.199 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:49: step 5400/223944 (epoch 1/3), loss = 5.715272 (0.213 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:53: step 5420/223944 (epoch 1/3), loss = 6.001243 (0.169 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:08:56: step 5440/223944 (epoch 1/3), loss = 3.465296 (0.128 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:01: step 5460/223944 (epoch 1/3), loss = 4.685808 (0.175 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:06: step 5480/223944 (epoch 1/3), loss = 4.505247 (0.144 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:11: step 5500/223944 (epoch 1/3), loss = 5.383235 (0.128 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:15: step 5520/223944 (epoch 1/3), loss = 4.623088 (0.330 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:19: step 5540/223944 (epoch 1/3), loss = 4.042351 (0.113 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:23: step 5560/223944 (epoch 1/3), loss = 6.240999 (0.208 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:26: step 5580/223944 (epoch 1/3), loss = 5.439878 (0.303 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:30: step 5600/223944 (epoch 1/3), loss = 4.821207 (0.105 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:34: step 5620/223944 (epoch 1/3), loss = 5.361830 (0.147 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:37: step 5640/223944 (epoch 1/3), loss = 4.591679 (0.188 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:42: step 5660/223944 (epoch 1/3), loss = 5.385324 (0.190 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:45: step 5680/223944 (epoch 1/3), loss = 4.424044 (0.098 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:49: step 5700/223944 (epoch 1/3), loss = 3.517099 (0.178 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:53: step 5720/223944 (epoch 1/3), loss = 4.899951 (0.199 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:09:57: step 5740/223944 (epoch 1/3), loss = 5.147255 (0.282 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:01: step 5760/223944 (epoch 1/3), loss = 4.899951 (0.236 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:05: step 5780/223944 (epoch 1/3), loss = 6.215135 (0.134 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:09: step 5800/223944 (epoch 1/3), loss = 2.762815 (0.093 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:13: step 5820/223944 (epoch 1/3), loss = 4.661451 (0.185 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:17: step 5840/223944 (epoch 1/3), loss = 4.415483 (0.161 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:20: step 5860/223944 (epoch 1/3), loss = 4.191694 (0.174 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:24: step 5880/223944 (epoch 1/3), loss = 6.849451 (0.247 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:29: step 5900/223944 (epoch 1/3), loss = 5.010884 (0.186 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:33: step 5920/223944 (epoch 1/3), loss = 5.094693 (0.215 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:36: step 5940/223944 (epoch 1/3), loss = 4.397908 (0.114 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:40: step 5960/223944 (epoch 1/3), loss = 5.534592 (0.114 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:44: step 5980/223944 (epoch 1/3), loss = 6.082096 (0.189 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:48: step 6000/223944 (epoch 1/3), loss = 6.056318 (0.207 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:53: step 6020/223944 (epoch 1/3), loss = 4.483489 (0.143 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:10:57: step 6040/223944 (epoch 1/3), loss = 6.442627 (0.205 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:01: step 6060/223944 (epoch 1/3), loss = 5.428736 (0.221 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:05: step 6080/223944 (epoch 1/3), loss = 5.162099 (0.181 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:09: step 6100/223944 (epoch 1/3), loss = 4.082985 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:12: step 6120/223944 (epoch 1/3), loss = 4.707798 (0.083 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:17: step 6140/223944 (epoch 1/3), loss = 5.902823 (0.235 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:21: step 6160/223944 (epoch 1/3), loss = 5.365307 (0.171 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:26: step 6180/223944 (epoch 1/3), loss = 4.047257 (0.191 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:30: step 6200/223944 (epoch 1/3), loss = 3.729632 (0.284 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:34: step 6220/223944 (epoch 1/3), loss = 4.910926 (0.258 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:39: step 6240/223944 (epoch 1/3), loss = 5.248530 (0.166 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:43: step 6260/223944 (epoch 1/3), loss = 5.979824 (0.215 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:47: step 6280/223944 (epoch 1/3), loss = 4.043399 (0.149 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:51: step 6300/223944 (epoch 1/3), loss = 6.745476 (0.159 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:54: step 6320/223944 (epoch 1/3), loss = 5.822939 (0.227 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:11:58: step 6340/223944 (epoch 1/3), loss = 5.635888 (0.175 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:02: step 6360/223944 (epoch 1/3), loss = 5.040775 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:06: step 6380/223944 (epoch 1/3), loss = 5.338939 (0.255 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:09: step 6400/223944 (epoch 1/3), loss = 3.735678 (0.242 sec/batch), lr: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 04:12:14: step 6420/223944 (epoch 1/3), loss = 5.864668 (0.184 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:17: step 6440/223944 (epoch 1/3), loss = 5.246961 (0.167 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:21: step 6460/223944 (epoch 1/3), loss = 5.734756 (0.142 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:25: step 6480/223944 (epoch 1/3), loss = 4.550629 (0.143 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:29: step 6500/223944 (epoch 1/3), loss = 4.968910 (0.105 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:33: step 6520/223944 (epoch 1/3), loss = 5.133187 (0.161 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:37: step 6540/223944 (epoch 1/3), loss = 4.766873 (0.184 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:42: step 6560/223944 (epoch 1/3), loss = 5.393078 (0.211 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:46: step 6580/223944 (epoch 1/3), loss = 5.775028 (0.207 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:50: step 6600/223944 (epoch 1/3), loss = 5.857060 (0.492 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:53: step 6620/223944 (epoch 1/3), loss = 4.711180 (0.157 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:12:57: step 6640/223944 (epoch 1/3), loss = 2.559580 (0.063 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:01: step 6660/223944 (epoch 1/3), loss = 5.151690 (0.205 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:06: step 6680/223944 (epoch 1/3), loss = 4.615300 (0.192 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:09: step 6700/223944 (epoch 1/3), loss = 4.832099 (0.110 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:12: step 6720/223944 (epoch 1/3), loss = 4.240853 (0.203 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:16: step 6740/223944 (epoch 1/3), loss = 5.650278 (0.299 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:20: step 6760/223944 (epoch 1/3), loss = 5.470939 (0.098 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:24: step 6780/223944 (epoch 1/3), loss = 6.628683 (0.505 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:27: step 6800/223944 (epoch 1/3), loss = 5.292129 (0.157 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:31: step 6820/223944 (epoch 1/3), loss = 4.067398 (0.174 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:35: step 6840/223944 (epoch 1/3), loss = 5.754241 (0.363 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:40: step 6860/223944 (epoch 1/3), loss = 5.093731 (0.099 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:44: step 6880/223944 (epoch 1/3), loss = 5.004481 (0.256 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:48: step 6900/223944 (epoch 1/3), loss = 4.232617 (0.188 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:53: step 6920/223944 (epoch 1/3), loss = 5.533287 (0.198 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:13:58: step 6940/223944 (epoch 1/3), loss = 4.141194 (0.166 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:02: step 6960/223944 (epoch 1/3), loss = 6.178845 (0.182 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:06: step 6980/223944 (epoch 1/3), loss = 5.175976 (0.207 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:10: step 7000/223944 (epoch 1/3), loss = 4.765873 (0.101 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:14: step 7020/223944 (epoch 1/3), loss = 5.291358 (0.518 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:18: step 7040/223944 (epoch 1/3), loss = 4.896336 (0.209 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:22: step 7060/223944 (epoch 1/3), loss = 5.852383 (0.251 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:26: step 7080/223944 (epoch 1/3), loss = 5.710549 (0.221 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:31: step 7100/223944 (epoch 1/3), loss = 5.172421 (0.202 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:34: step 7120/223944 (epoch 1/3), loss = 4.494981 (0.112 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:37: step 7140/223944 (epoch 1/3), loss = 5.484977 (0.148 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:41: step 7160/223944 (epoch 1/3), loss = 5.021247 (0.220 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:45: step 7180/223944 (epoch 1/3), loss = 4.966404 (0.175 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:49: step 7200/223944 (epoch 1/3), loss = 4.832615 (0.102 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:52: step 7220/223944 (epoch 1/3), loss = 5.961574 (0.230 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:14:57: step 7240/223944 (epoch 1/3), loss = 5.666401 (0.247 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:01: step 7260/223944 (epoch 1/3), loss = 5.167082 (0.174 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:05: step 7280/223944 (epoch 1/3), loss = 5.184348 (0.213 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:08: step 7300/223944 (epoch 1/3), loss = 5.149603 (0.254 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:12: step 7320/223944 (epoch 1/3), loss = 3.924951 (0.105 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:16: step 7340/223944 (epoch 1/3), loss = 6.102376 (0.224 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:19: step 7360/223944 (epoch 1/3), loss = 4.059061 (0.218 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:24: step 7380/223944 (epoch 1/3), loss = 4.460978 (0.230 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:28: step 7400/223944 (epoch 1/3), loss = 6.427925 (0.215 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:31: step 7420/223944 (epoch 1/3), loss = 5.680390 (0.268 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:35: step 7440/223944 (epoch 1/3), loss = 5.440767 (0.305 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:40: step 7460/223944 (epoch 1/3), loss = 5.167678 (0.219 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:44: step 7480/223944 (epoch 1/3), loss = 5.182076 (0.315 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:48: step 7500/223944 (epoch 1/3), loss = 5.160238 (0.166 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:52: step 7520/223944 (epoch 1/3), loss = 3.710919 (0.111 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:15:57: step 7540/223944 (epoch 1/3), loss = 3.504505 (0.254 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:01: step 7560/223944 (epoch 1/3), loss = 2.871749 (0.139 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:05: step 7580/223944 (epoch 1/3), loss = 4.054382 (0.279 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:09: step 7600/223944 (epoch 1/3), loss = 4.627867 (0.112 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:13: step 7620/223944 (epoch 1/3), loss = 4.740855 (0.260 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:17: step 7640/223944 (epoch 1/3), loss = 3.576721 (0.162 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:22: step 7660/223944 (epoch 1/3), loss = 5.982826 (0.323 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:26: step 7680/223944 (epoch 1/3), loss = 4.329582 (0.191 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:30: step 7700/223944 (epoch 1/3), loss = 5.160314 (0.217 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:34: step 7720/223944 (epoch 1/3), loss = 5.503036 (0.139 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:37: step 7740/223944 (epoch 1/3), loss = 4.133897 (0.236 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:41: step 7760/223944 (epoch 1/3), loss = 5.766977 (0.122 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:46: step 7780/223944 (epoch 1/3), loss = 4.882493 (0.160 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:50: step 7800/223944 (epoch 1/3), loss = 5.649160 (0.215 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:54: step 7820/223944 (epoch 1/3), loss = 5.205716 (0.124 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:16:59: step 7840/223944 (epoch 1/3), loss = 4.422138 (0.240 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:03: step 7860/223944 (epoch 1/3), loss = 2.962644 (0.303 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:07: step 7880/223944 (epoch 1/3), loss = 5.817817 (0.272 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:10: step 7900/223944 (epoch 1/3), loss = 4.600993 (0.194 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:14: step 7920/223944 (epoch 1/3), loss = 5.106110 (0.246 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:18: step 7940/223944 (epoch 1/3), loss = 5.563082 (0.292 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:23: step 7960/223944 (epoch 1/3), loss = 5.425874 (0.476 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:27: step 7980/223944 (epoch 1/3), loss = 5.793387 (0.161 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:31: step 8000/223944 (epoch 1/3), loss = 4.656516 (0.144 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:35: step 8020/223944 (epoch 1/3), loss = 4.550475 (0.239 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:39: step 8040/223944 (epoch 1/3), loss = 4.960384 (0.099 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:43: step 8060/223944 (epoch 1/3), loss = 5.168966 (0.281 sec/batch), lr: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 04:17:47: step 8080/223944 (epoch 1/3), loss = 4.986622 (0.209 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:51: step 8100/223944 (epoch 1/3), loss = 5.318104 (0.156 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:55: step 8120/223944 (epoch 1/3), loss = 5.267671 (0.279 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:17:59: step 8140/223944 (epoch 1/3), loss = 3.780145 (0.130 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:03: step 8160/223944 (epoch 1/3), loss = 5.233938 (0.171 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:07: step 8180/223944 (epoch 1/3), loss = 4.835152 (0.270 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:12: step 8200/223944 (epoch 1/3), loss = 4.904494 (0.194 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:15: step 8220/223944 (epoch 1/3), loss = 4.622981 (0.151 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:19: step 8240/223944 (epoch 1/3), loss = 5.732751 (0.213 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:23: step 8260/223944 (epoch 1/3), loss = 5.265625 (0.265 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:27: step 8280/223944 (epoch 1/3), loss = 5.137087 (0.269 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:31: step 8300/223944 (epoch 1/3), loss = 3.434994 (0.105 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:34: step 8320/223944 (epoch 1/3), loss = 5.134983 (0.160 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:38: step 8340/223944 (epoch 1/3), loss = 3.978881 (0.151 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:42: step 8360/223944 (epoch 1/3), loss = 4.849987 (0.166 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:46: step 8380/223944 (epoch 1/3), loss = 3.613521 (0.084 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:50: step 8400/223944 (epoch 1/3), loss = 4.501763 (0.199 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:54: step 8420/223944 (epoch 1/3), loss = 4.157974 (0.142 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:18:57: step 8440/223944 (epoch 1/3), loss = 5.210610 (0.178 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:02: step 8460/223944 (epoch 1/3), loss = 4.200842 (0.097 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:06: step 8480/223944 (epoch 1/3), loss = 5.708595 (0.172 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:09: step 8500/223944 (epoch 1/3), loss = 4.246837 (0.173 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:13: step 8520/223944 (epoch 1/3), loss = 5.300106 (0.227 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:17: step 8540/223944 (epoch 1/3), loss = 5.064242 (0.171 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:21: step 8560/223944 (epoch 1/3), loss = 4.949618 (0.161 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:24: step 8580/223944 (epoch 1/3), loss = 5.381800 (0.200 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:29: step 8600/223944 (epoch 1/3), loss = 4.571829 (0.291 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:33: step 8620/223944 (epoch 1/3), loss = 4.847520 (0.336 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:37: step 8640/223944 (epoch 1/3), loss = 4.158557 (0.135 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:41: step 8660/223944 (epoch 1/3), loss = 5.270114 (0.201 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:45: step 8680/223944 (epoch 1/3), loss = 4.208855 (0.268 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:49: step 8700/223944 (epoch 1/3), loss = 3.963874 (0.212 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:53: step 8720/223944 (epoch 1/3), loss = 4.968061 (0.169 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:19:56: step 8740/223944 (epoch 1/3), loss = 4.354861 (0.134 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:00: step 8760/223944 (epoch 1/3), loss = 4.336308 (0.136 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:04: step 8780/223944 (epoch 1/3), loss = 3.331549 (0.071 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:08: step 8800/223944 (epoch 1/3), loss = 5.151089 (0.154 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:12: step 8820/223944 (epoch 1/3), loss = 5.816113 (0.259 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:16: step 8840/223944 (epoch 1/3), loss = 4.959955 (0.129 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:20: step 8860/223944 (epoch 1/3), loss = 5.209847 (0.153 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:24: step 8880/223944 (epoch 1/3), loss = 5.288923 (0.189 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:27: step 8900/223944 (epoch 1/3), loss = 4.568789 (0.155 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:31: step 8920/223944 (epoch 1/3), loss = 5.898717 (0.213 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:36: step 8940/223944 (epoch 1/3), loss = 4.809091 (0.244 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:39: step 8960/223944 (epoch 1/3), loss = 3.828379 (0.163 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:43: step 8980/223944 (epoch 1/3), loss = 4.885903 (0.227 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:47: step 9000/223944 (epoch 1/3), loss = 4.118246 (0.115 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:51: step 9020/223944 (epoch 1/3), loss = 3.812119 (0.138 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:55: step 9040/223944 (epoch 1/3), loss = 5.591288 (0.243 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:20:59: step 9060/223944 (epoch 1/3), loss = 3.240325 (0.092 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:02: step 9080/223944 (epoch 1/3), loss = 4.491414 (0.101 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:07: step 9100/223944 (epoch 1/3), loss = 3.972293 (0.139 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:10: step 9120/223944 (epoch 1/3), loss = 5.714285 (0.117 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:14: step 9140/223944 (epoch 1/3), loss = 4.735540 (0.169 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:17: step 9160/223944 (epoch 1/3), loss = 5.669231 (0.391 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:21: step 9180/223944 (epoch 1/3), loss = 4.544544 (0.112 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:25: step 9200/223944 (epoch 1/3), loss = 5.916111 (0.380 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:29: step 9220/223944 (epoch 1/3), loss = 2.416336 (0.079 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:33: step 9240/223944 (epoch 1/3), loss = 4.862585 (0.124 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:37: step 9260/223944 (epoch 1/3), loss = 4.296778 (0.126 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:41: step 9280/223944 (epoch 1/3), loss = 4.077679 (0.169 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:45: step 9300/223944 (epoch 1/3), loss = 6.056770 (0.198 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:49: step 9320/223944 (epoch 1/3), loss = 4.139128 (0.140 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:53: step 9340/223944 (epoch 1/3), loss = 2.867185 (0.252 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:21:58: step 9360/223944 (epoch 1/3), loss = 5.520070 (0.128 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:03: step 9380/223944 (epoch 1/3), loss = 5.279646 (0.151 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:06: step 9400/223944 (epoch 1/3), loss = 4.710492 (0.208 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:10: step 9420/223944 (epoch 1/3), loss = 3.893528 (0.170 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:14: step 9440/223944 (epoch 1/3), loss = 5.922250 (0.179 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:17: step 9460/223944 (epoch 1/3), loss = 4.568889 (0.165 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:22: step 9480/223944 (epoch 1/3), loss = 4.758632 (0.165 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:25: step 9500/223944 (epoch 1/3), loss = 4.038652 (0.151 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:30: step 9520/223944 (epoch 1/3), loss = 4.304145 (0.232 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:33: step 9540/223944 (epoch 1/3), loss = 3.392631 (0.088 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:38: step 9560/223944 (epoch 1/3), loss = 5.012053 (0.138 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:42: step 9580/223944 (epoch 1/3), loss = 3.604531 (0.159 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:46: step 9600/223944 (epoch 1/3), loss = 5.392628 (0.301 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:50: step 9620/223944 (epoch 1/3), loss = 5.354415 (0.125 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:54: step 9640/223944 (epoch 1/3), loss = 5.965831 (0.407 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:22:58: step 9660/223944 (epoch 1/3), loss = 5.228004 (0.336 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:02: step 9680/223944 (epoch 1/3), loss = 6.197073 (0.200 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:06: step 9700/223944 (epoch 1/3), loss = 4.565179 (0.193 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:10: step 9720/223944 (epoch 1/3), loss = 5.662997 (0.209 sec/batch), lr: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 04:23:13: step 9740/223944 (epoch 1/3), loss = 5.214248 (0.286 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:17: step 9760/223944 (epoch 1/3), loss = 5.054987 (0.252 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:21: step 9780/223944 (epoch 1/3), loss = 3.018207 (0.111 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:25: step 9800/223944 (epoch 1/3), loss = 5.018272 (0.382 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:29: step 9820/223944 (epoch 1/3), loss = 5.250906 (0.100 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:33: step 9840/223944 (epoch 1/3), loss = 4.964127 (0.218 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:38: step 9860/223944 (epoch 1/3), loss = 4.443533 (0.250 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:42: step 9880/223944 (epoch 1/3), loss = 3.038129 (0.218 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:46: step 9900/223944 (epoch 1/3), loss = 3.399281 (0.161 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:49: step 9920/223944 (epoch 1/3), loss = 4.698596 (0.118 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:53: step 9940/223944 (epoch 1/3), loss = 5.335158 (0.251 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:23:57: step 9960/223944 (epoch 1/3), loss = 4.112195 (0.135 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:00: step 9980/223944 (epoch 1/3), loss = 4.217911 (0.186 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:04: step 10000/223944 (epoch 1/3), loss = 4.233815 (0.216 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:08: step 10020/223944 (epoch 1/3), loss = 4.745556 (0.169 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:12: step 10040/223944 (epoch 1/3), loss = 3.428910 (0.090 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:16: step 10060/223944 (epoch 1/3), loss = 5.709951 (0.325 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:20: step 10080/223944 (epoch 1/3), loss = 5.842975 (0.197 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:24: step 10100/223944 (epoch 1/3), loss = 4.390155 (0.235 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:28: step 10120/223944 (epoch 1/3), loss = 4.187982 (0.147 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:32: step 10140/223944 (epoch 1/3), loss = 4.711377 (0.201 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:36: step 10160/223944 (epoch 1/3), loss = 4.573533 (0.422 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:40: step 10180/223944 (epoch 1/3), loss = 5.162485 (0.157 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:44: step 10200/223944 (epoch 1/3), loss = 4.363544 (0.139 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:48: step 10220/223944 (epoch 1/3), loss = 3.666080 (0.181 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:51: step 10240/223944 (epoch 1/3), loss = 5.305597 (0.345 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:55: step 10260/223944 (epoch 1/3), loss = 4.136525 (0.118 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:24:59: step 10280/223944 (epoch 1/3), loss = 4.640678 (0.133 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:02: step 10300/223944 (epoch 1/3), loss = 4.998194 (0.112 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:07: step 10320/223944 (epoch 1/3), loss = 6.141468 (0.165 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:11: step 10340/223944 (epoch 1/3), loss = 5.702281 (0.746 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:15: step 10360/223944 (epoch 1/3), loss = 5.262998 (0.130 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:18: step 10380/223944 (epoch 1/3), loss = 5.900783 (0.134 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:21: step 10400/223944 (epoch 1/3), loss = 5.103487 (0.136 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:25: step 10420/223944 (epoch 1/3), loss = 4.080504 (0.246 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:29: step 10440/223944 (epoch 1/3), loss = 3.730189 (0.132 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:33: step 10460/223944 (epoch 1/3), loss = 4.901317 (0.161 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:37: step 10480/223944 (epoch 1/3), loss = 4.626812 (0.168 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:42: step 10500/223944 (epoch 1/3), loss = 5.774414 (0.542 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:45: step 10520/223944 (epoch 1/3), loss = 5.722061 (0.449 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:49: step 10540/223944 (epoch 1/3), loss = 4.259808 (0.174 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:52: step 10560/223944 (epoch 1/3), loss = 5.155731 (0.311 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:25:56: step 10580/223944 (epoch 1/3), loss = 2.969311 (0.070 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:26:00: step 10600/223944 (epoch 1/3), loss = 4.443433 (0.249 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:26:04: step 10620/223944 (epoch 1/3), loss = 4.584079 (0.295 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:26:08: step 10640/223944 (epoch 1/3), loss = 3.950824 (0.132 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:26:11: step 10660/223944 (epoch 1/3), loss = 5.292126 (0.268 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:26:16: step 10680/223944 (epoch 1/3), loss = 3.412605 (0.113 sec/batch), lr: 0.001000\n",
      "2019-05-28 04:26:20: step 10700/223944 (epoch 1/3), loss = 5.530400 (0.280 sec/batch), lr: 0.001000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.30 GiB (GPU 0; 15.75 GiB total capacity; 13.72 GiB already allocated; 798.88 MiB free; 280.86 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/p3_16x/GitBackgroundModel/summarize-radiology-findings/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3_16x/GitBackgroundModel/summarize-radiology-findings/model/trainer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, eval)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3_16x/GitBackgroundModel/summarize-radiology-findings/model/copy_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt_in, bg)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# then decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mout_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbg_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3_16x/GitBackgroundModel/summarize-radiology-findings/model/copy_model.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, dec_inputs, dec_hidden, ctx, ctx_tokens, ctx_mask, bg_h, inference)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# combine in log space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mcombined_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_copier_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_dec_prob\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# output dec_hidden for future decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.30 GiB (GPU 0; 15.75 GiB total capacity; 13.72 GiB already allocated; 798.88 MiB free; 280.86 MiB cached)"
     ]
    }
   ],
   "source": [
    "%run train.py --id IA_Model_v1 --data_dir dataset/ia-patients/utku_reduced_data/ --batch_size 2 --num_epoch 3 --lr 0.001 --background --decay_epoch 15 --emb_dim 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tagucci/pythonrouge.git\n",
      "  Cloning https://github.com/tagucci/pythonrouge.git to /tmp/pip-req-build-wo3fnghr\n",
      "Building wheels for collected packages: pythonrouge\n",
      "  Running setup.py bdist_wheel for pythonrouge ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-jaelxltv/wheels/fd/ff/be/6716935d513fa8656ab185cb0aa70aed382b72dda42bf09c95\n",
      "Successfully built pythonrouge\n",
      "\u001b[31msagemaker 1.18.5 has requirement requests<2.21,>=2.20.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mdocker-compose 1.23.2 has requirement requests!=2.11.0,!=2.12.2,!=2.18.0,<2.21,>=2.6.1, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pythonrouge\n",
      "Successfully installed pythonrouge-0.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tagucci/pythonrouge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 15:58:04 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   36C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 2480\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo kill -9 6154 // sudo kill -9 6154"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
