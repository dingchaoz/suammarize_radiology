{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Install\n",
    "```\n",
    "pip install torch torchvision --user\n",
    "pip install tqdm --user\n",
    "pip install git+https://github.com/tagucci/pythonrouge.git\n",
    "```\n",
    "\n",
    "If `pythonrouge` gives an error about `perl` saying `non-zero exit status 2`:\n",
    "```\n",
    "apt-get install libxml-parser-perl\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1012M\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  1.7M Mar 12 19:08 \u001b[0m\u001b[01;32mConvert_IA_data_to_jsonl.ipynb\u001b[0m*\r\n",
      "drwxr-xr-x 3 ubuntu ubuntu  4.0K Mar 12 19:11 \u001b[01;34mdata\u001b[0m/\r\n",
      "drwxr-xr-x 7 ubuntu ubuntu  4.0K Mar 12 19:09 \u001b[01;34mdataset\u001b[0m/\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu   280 Mar 11 17:22 \u001b[01;32mdownload.sh\u001b[0m*\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  3.0K Mar 11 17:22 \u001b[01;32meval.py\u001b[0m*\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1007M Mar 12 19:00 ia_data_all.jsonl\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu   603 Mar 11 17:22 \u001b[01;32mLICENSE\u001b[0m*\r\n",
      "drwxr-xr-x 3 ubuntu ubuntu  4.0K Mar 12 19:18 \u001b[01;34mmodel\u001b[0m/\r\n",
      "-rw------- 1 ubuntu ubuntu  619K Mar 14 12:42 nohup_Nan.out\r\n",
      "-rw------- 1 ubuntu ubuntu  1.8M Mar 17 05:01 nohup.out\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  4.1K Mar 11 17:22 \u001b[01;32mprepare_vocab.py\u001b[0m*\r\n",
      "drwxr-xr-x 2 ubuntu ubuntu  4.0K Mar 11 17:22 \u001b[01;34mpretrained\u001b[0m/\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  4.4K Mar 11 17:22 \u001b[01;32mREADME.md\u001b[0m*\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  2.9K Mar 11 17:22 \u001b[01;32mrun.py\u001b[0m*\r\n",
      "drwxrwxr-x 3 ubuntu ubuntu  4.0K Mar 12 19:19 \u001b[01;34msaved_models\u001b[0m/\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu   91K Mar 13 06:55 \u001b[01;32mtrain_IA_model.ipynb\u001b[0m*\r\n",
      "-rwxr-xr-x 1 ubuntu ubuntu  8.1K Mar 11 17:22 \u001b[01;32mtrain.py\u001b[0m*\r\n",
      "drwxr-xr-x 3 ubuntu ubuntu  4.0K Mar 12 18:29 \u001b[01;34mutils\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check GPU Presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = torch.cuda.current_device()\n",
    "torch.cuda.device(ind)\n",
    "print(\"# of GPUs:  {}\".format(torch.cuda.device_count()))\n",
    "print(\"GPU index:  {}\".format(ind))\n",
    "print(\"GPU name:   {}\".format(torch.cuda.get_device_name(ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Word Embeddings and Vocabulary\n",
    "- Create a folder called `ia-patients` under `dataset`\n",
    "- Converted `jsonl` data goes to this folder\n",
    "- 3 files needed: `train.jsonl`, `dev.jsonl`, `test.jsonl`\n",
    "- More training parameters are available in `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory dataset/vocab do not exist; creating...\n",
      "loading files...\n",
      "67106584 tokens from 421862 examples loaded from dataset/ia-patients//train.jsonl.\n",
      "14384492 tokens from 90399 examples loaded from dataset/ia-patients//dev.jsonl.\n",
      "14371443 tokens from 90398 examples loaded from dataset/ia-patients//test.jsonl.\n",
      "loading glove...\n",
      "1 words loaded from glove.\n",
      "building vocab...\n",
      "vocab built with 5/453276 words.\n",
      "calculating oov...\n",
      "train oov: 67106584/67106584 (100.00%)\n",
      "dev oov: 14384492/14384492 (100.00%)\n",
      "test oov: 14371443/14371443 (100.00%)\n",
      "building embeddings...\n",
      "embedding size: 5 x 100\n",
      "dumping to files...\n",
      "all done.\n"
     ]
    }
   ],
   "source": [
    "%run prepare_vocab.py dataset/ia-patients/ dataset/vocab --glove_dir dataset/glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 417194 loaded from file\n",
      "Loading data from dataset/ia-patients/ with batch size 10...\n",
      "14930 batches created for dataset/ia-patients//trainu.jsonl.\n",
      "1867 batches created for dataset/ia-patients//devu.jsonl.\n",
      "Config saved to file ./saved_models/IA_Model_v1/config.json\n",
      "Overwriting old vocab file at ./saved_models/IA_Model_v1/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/ia-patients/\n",
      "\tvocab_dir : dataset/vocab\n",
      "\thidden_dim : 150\n",
      "\temb_dim : 300\n",
      "\tnum_layers : 2\n",
      "\temb_dropout : 0.5\n",
      "\tdropout : 0.5\n",
      "\tlower : True\n",
      "\tmax_dec_len : 80\n",
      "\tbeam_size : 5\n",
      "\ttop : 1000000\n",
      "\ttrain_data : trainu\n",
      "\tdev_data : devu\n",
      "\tattn_type : mlp\n",
      "\tcov : False\n",
      "\tcov_alpha : 0\n",
      "\tcov_loss_epoch : 0\n",
      "\tbackground : False\n",
      "\tconcat_background : False\n",
      "\tuse_bleu : False\n",
      "\tsample_train : 1.0\n",
      "\tlr : 0.001\n",
      "\tlr_decay : 0.9\n",
      "\tdecay_epoch : 30\n",
      "\toptim : adam\n",
      "\tnum_epoch : 5\n",
      "\tbatch_size : 10\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : IA_Model_v1\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tvocab_size : 417194\n",
      "\tmodel_save_dir : ./saved_models/IA_Model_v1\n",
      "\n",
      "\n",
      "Building Seq2Seq with Copy model ...\n",
      "Using mlp attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Update coverage loss weight to be 0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.77 GiB (GPU 0; 15.75 GiB total capacity; 13.93 GiB already allocated; 524.94 MiB free; 344.96 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/p3_16x/Background_Model/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3_16x/Background_Model/model/trainer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch, eval)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3_16x/Background_Model/model/copy_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt_in, bg)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# then decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mout_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbg_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3_16x/Background_Model/model/copy_model.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, dec_inputs, dec_hidden, ctx, ctx_tokens, ctx_mask, bg_h, inference)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m#torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# combine in log space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mcombined_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_copier_probs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_dec_prob\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder_logits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# some shit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mcombined_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.77 GiB (GPU 0; 15.75 GiB total capacity; 13.93 GiB already allocated; 524.94 MiB free; 344.96 MiB cached)"
     ]
    }
   ],
   "source": [
    "%run train.py --id IA_Model_v1 --data_dir dataset/ia-patients/ --batch_size 10 --num_epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tagucci/pythonrouge.git\n",
      "  Cloning https://github.com/tagucci/pythonrouge.git to /tmp/pip-req-build-wo3fnghr\n",
      "Building wheels for collected packages: pythonrouge\n",
      "  Running setup.py bdist_wheel for pythonrouge ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-jaelxltv/wheels/fd/ff/be/6716935d513fa8656ab185cb0aa70aed382b72dda42bf09c95\n",
      "Successfully built pythonrouge\n",
      "\u001b[31msagemaker 1.18.5 has requirement requests<2.21,>=2.20.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mdocker-compose 1.23.2 has requirement requests!=2.11.0,!=2.12.2,!=2.18.0,<2.21,>=2.6.1, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pythonrouge\n",
      "Successfully installed pythonrouge-0.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tagucci/pythonrouge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 15:58:04 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   36C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 2480\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo kill -9 6154 // sudo kill -9 6154"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
